<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>deviate from the norm</title>
    <meta name="description" content="">
    <meta name="author" content="Ernő Gólya">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://ernogolya.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://ernogolya.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://ernogolya.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://ernogolya.github.io/theme/local.css" rel="stylesheet">
    <link href="https://ernogolya.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://ernogolya.github.io">deviate from the norm</a>

        <div class="nav-collapse">
        <ul class="nav">
            
            <li><a href="https://ernogolya.github.io/pages/about/">About</a></li>
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


    <div class='article'>
        <div class="content-title">
            <a href="https://ernogolya.github.io/random_forest/"><h1>Machine Learning for Data Analysis: Random Forests</h1></a>
k 30 augusztus 2016

by <a class="url fn" href="https://ernogolya.github.io/author/erno-golya.html">Ernő Gólya</a>
 


 
        </div>
        
        <div><p>The decision tree method introduced briefly in the <a href="/decision_tree/">previous post</a> represents a powerful approach for making predictions based on exploring how many variables can predict a particular target. Decision trees are easy to interpret and visualize and can potentially uncover patterns in our data that can not be easily identified through traditional regression methods. However, even small changes in the data can lead to different results and they are not very reproducible on future data. </p>
<p>Now, let's move on to a related machine learning method, known as Random Forest. This data mining algorithm is a versatile method capable of performing both regression and classification tasks. It can handle a large number of features, and it is helpful for estimating which of our variables are important in the underlying data being modeled.<br />
It belongs to a larger class of machine learning algorithms called ensemble methods which involve the combination of several models to solve a single prediction problem. Random forest aggregates classification (or regression) trees by growing multiple trees (decision tree forest). It works by generating multiple models which make predictions independently, then those predictions are combined into a single prediction. The trees generated are used to collectively rank the importance of variables in predicting our target of interest rather than being interpreted themselves. Thus we get a sense of the most important predictive variables but not necessarily their relationships to one another. To classify a new object based on attributes, each tree gives a classification, the tree “votes” for that class. The forest chooses the classification having the most votes over all the trees in the forest. By trying lots of decision tree variations we can examine which variables are working best or worst in each tree (feature selection). When a certain tree uses one variable and another doesn't, we can compare the effect of the inclusion/exclusion of that variable. 
Random forest is great for classification. It can be used to make predictions for categories with multiple possible values. </p>
<p>Random forest can be prone to overfitting, especially when working with relatively small datasets, such as the extract from the <a href="https://gapminder.org">Gapminder</a> dataset we are going to use to generate a random forest. </p>
<h3>Running a Random Forest</h3>
<p>Lets take a look at the results of the random forest analysis performed to evaluate the importance of a series of explanatory variables in predicting our binary, categorical response variable (<code>u5_abovemedian</code>, converted from child mortality rate using the global median of the under5mort variable as a cut point).
The following explanatory variables were included as possible contributors to the random forest model: <em>mean years in school for women, per capita expenditure on health, income per person, estimated HIV prevalence, urban rate, mean age at 1st marriage of women, corruption perception index, access to improved sanitation facilities, access to improved drinking water sources and teen fertility rate</em>.</p>
<div class="highlight"><pre>Shape of the training and test samples
(63, 10)
(43, 10)
(63,)
(43,)
</pre></div>


<p>Much of the code is similar to the code we had written for individual decision trees. The figure above shows the shape of the training and test samples. 63 observations, 60% of the dataset is used as training set. The remaining 40% or 43 observations are used as test set. There are 10 predictors or response variables used in this example. The Random Forest Classifier is initialized with 30 estimators, that is, the random forest is comprised of 30 trees.</p>
<div class="highlight"><pre>Confusion matrix
[[24  0]
 [ 2 17]]
</pre></div>


<p>The figure above shows the output of confusion matrix. The diagonal 24, 17 reflects the number of true negatives and true positives respectively. The diagonal 2, 0 reflects the false negatives and false positives for the categorized variable child mortality.</p>
<div class="highlight"><pre>Accuracy score
0.953488372093
</pre></div>


<p>The overall accuracy for the forest is 0.95. So 95% of the countries were classified correctly as child mortality rate being above or below the world average.</p>
<h3>Relative importance of features</h3>
<p>The most helpful information to be obtained from a forest is the measured importance for each explanatory variable (feature), based on how many splits each has produced in the 30 tree ensemble. </p>
<div class="highlight"><pre>Feature ranking:
1. sanit_pc: (0.154530)
2. watersource_pc: (0.151709)
3. incomeperperson: (0.134865)
4. healthexpend: (0.132546)
5. womenschool: (0.127546)
6. ageofmarriage: (0.082550)
7. corrupt_perc: (0.070926)
8. teenfertility: (0.060741)
9. urbanrate: (0.053461)
10. hivrate: (0.031126)
</pre></div>


<p><img alt="feature_importance" src="https://ernogolya.github.io/images/MachineLearning/feature_importance.png" /></p>
<p>The table and graph show the generated feature importance scores calculated from the forest of trees that we have grown. As we can see the variables with the highest important score are access to improved sanitation facilities (0.15), access to improved drinking water sources (0.15) and income per person (0.13) while the variable with the lowest important score is estimated HIV prevalence at 0.031. </p>
<h3>Correct classification rate for different number of trees</h3>
<p>To determine what growing larger number of trees brings us in terms of correct classification, we can use code that builds different numbers of trees, and provides the correct classification rate for each. We have used 30 trees in our random forest experiment to achieve this. The figure below shows the accuracy score plotted against the number of trees. </p>
<p><img alt="accuracy_of_trees" src="https://ernogolya.github.io/images/MachineLearning/accuracy_of_trees.png" /></p>
<p>As we can see, with only one tree the accuracy is about 84%, and it climbs up to 95% (at about 12-13 trees), with the subsequent growing of trees adding no more to the overall accuracy of the model.</p>
<h3>Python code</h3>
<div class="highlight"><pre><span class="c"># -*- coding: utf-8 -*-</span>
<span class="c">#</span>
<span class="c"># Created on Aug 29 2016</span>
<span class="c"># @author: Ernő Gólya</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">Series</span><span class="p">,</span> <span class="n">DataFrame</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234567890</span><span class="p">)</span>

<span class="c"># data management</span>
<span class="n">inp_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&quot;custom_gapminder_3.csv&quot;</span><span class="p">)</span>

<span class="n">variables</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;under5mort&#39;</span><span class="p">,</span> <span class="s">&#39;womenschool&#39;</span><span class="p">,</span> <span class="s">&#39;healthexpend&#39;</span><span class="p">,</span> <span class="s">&#39;incomeperperson&#39;</span><span class="p">,</span> <span class="s">&#39;hivrate&#39;</span><span class="p">,</span> <span class="s">&#39;urbanrate&#39;</span><span class="p">,</span>
                     <span class="s">&#39;ageofmarriage&#39;</span><span class="p">,</span> <span class="s">&#39;corrupt_perc&#39;</span><span class="p">,</span> <span class="s">&#39;sanit_pc&#39;</span><span class="p">,</span> <span class="s">&#39;watersource_pc&#39;</span><span class="p">,</span> <span class="s">&#39;teenfertility&#39;</span><span class="p">]</span>

<span class="n">inp_data1</span> <span class="o">=</span> <span class="n">inp_data</span><span class="p">[</span><span class="n">variables</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">inp_data1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c"># convert selected variables to numeric</span>
<span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">:</span>
    <span class="n">data</span><span class="p">[</span><span class="n">variable</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">variable</span><span class="p">],</span><span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span>

<span class="c"># delete observations with missing value(s)</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c"># create u5_abovemedian variable (value = 1 if under5mort &gt; under5median, othervise value = 0)</span>
<span class="n">under5median</span> <span class="o">=</span> <span class="n">data2</span><span class="p">[</span><span class="s">&#39;under5mort&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">u5_abovemedian</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s">&#39;under5mort&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">under5median</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="n">data3</span> <span class="o">=</span> <span class="n">data2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">data3</span><span class="p">[</span><span class="s">&#39;u5_abovemedian&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data3</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">u5_abovemedian</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">print</span> <span class="s">&quot;Median of child mortality rate&quot;</span>
<span class="k">print</span> <span class="n">under5median</span><span class="p">,</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>

<span class="c"># create list of predictors</span>
<span class="n">pred_variables</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">variables</span> <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="s">&#39;under5mort&#39;</span><span class="p">]</span>

<span class="c"># split dataset into training and testing sets</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="n">data3</span><span class="p">[</span><span class="n">pred_variables</span><span class="p">]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">data3</span><span class="o">.</span><span class="n">u5_abovemedian</span>
<span class="n">pred_train</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">,</span> <span class="n">tar_train</span><span class="p">,</span> <span class="n">tar_test</span>  <span class="o">=</span>   <span class="n">train_test_split</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">4</span><span class="p">)</span>

<span class="k">print</span> <span class="s">&quot;Shape of the training and test samples&quot;</span>
<span class="k">print</span> <span class="n">pred_train</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">pred_test</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">tar_train</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="n">tar_test</span><span class="o">.</span><span class="n">shape</span>

<span class="c"># build model on training set</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span><span class="n">tar_train</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_test</span><span class="p">)</span>

<span class="c"># print confusion matrix and accuracy score</span>
<span class="k">print</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">Confusion matrix&quot;</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">tar_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="k">print</span> <span class="n">cm</span>
<span class="k">print</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">Accuracy score&quot;</span>
<span class="k">print</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">tar_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;&quot;</span>

<span class="c"># fit an Extra Trees model to the data</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">tar_train</span><span class="p">)</span>

<span class="c"># display the relative importance of each attribute</span>
<span class="k">print</span> <span class="s">&quot;Relative importance of features&quot;</span>
<span class="k">print</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="k">print</span> <span class="s">&quot;&quot;</span>

<span class="c"># Running a different number of trees and see the effect</span>
<span class="c"># of that on the accuracy of the prediction</span>
<span class="n">trees</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">)):</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">tar_train</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pred_test</span><span class="p">)</span>
    <span class="n">accuracy</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">tar_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="c"># print the feature ranking</span>
<span class="n">importances</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">&quot;Feature ranking:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">predictors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%d</span><span class="s">. </span><span class="si">%s</span><span class="s">: (</span><span class="si">%f</span><span class="s">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">predictors</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]],</span> <span class="n">importances</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]])</span>
<span class="n">indices_rev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">predictors</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices_rev</span><span class="p">]</span>

<span class="c"># plot the feature importances of the forest</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">opacity</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Feature importances&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">predictors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">importances</span><span class="p">[</span><span class="n">indices_rev</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s">&quot;center&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">opacity</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">predictors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s">&#39;horizontal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">predictors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;Relative Importance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    

<span class="k">print</span> <span class="s">&quot;&quot;</span>

<span class="n">fig</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trees</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;Trees&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Accuracy of Trees&#39;</span><span class="p">);</span>
</pre></div></div>
        <hr />
    </div>
		

 
        

 

    <div class='article'>
        <a href="https://ernogolya.github.io/decision_tree/"><h2>Machine Learning for Data Analysis: Decision Trees</h2></a>
        <div class= "well small"> p 26 augusztus 2016

by <a class="url fn" href="https://ernogolya.github.io/author/erno-golya.html">Ernő Gólya</a>
 


 </div>
        <div class="summary"><p>Decision tree is a type of supervised learning algorithm that is mostly used in classification problems. It is a type of data mining method that allows us to explore the presence of potentially complicated interactions within our data by creating subgroups.</p> <a class="btn btn-info xsmall" href="https://ernogolya.github.io/decision_tree/">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://ernogolya.github.io/logistic_regression/"><h2>Regression Modeling in Practice: Logistic Regression Model</h2></a>
        <div class= "well small"> h 22 augusztus 2016

by <a class="url fn" href="https://ernogolya.github.io/author/erno-golya.html">Ernő Gólya</a>
 


 </div>
        <div class="summary"><p>Logistic regression is another form of the linear regression model, so the basic idea is the same as a multiple regression analysis. But, unlike the multiple regression model, the logistic regression model is designed to test binary response variables.</p> <a class="btn btn-info xsmall" href="https://ernogolya.github.io/logistic_regression/">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://ernogolya.github.io/multiple_regression/"><h2>Regression Modeling in Practice: Multiple Regression Model</h2></a>
        <div class= "well small"> v 21 augusztus 2016

by <a class="url fn" href="https://ernogolya.github.io/author/erno-golya.html">Ernő Gólya</a>
 


 </div>
        <div class="summary"><p>The results of my linear regression model indicated that the level of female education in a country was significantly and negatively associated with the number of children dying under the age of five. But what other variables might explain more of this association and the variability in child mortality?</p> <a class="btn btn-info xsmall" href="https://ernogolya.github.io/multiple_regression/">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://ernogolya.github.io/basic_linear_regression/"><h2>Regression Modeling in Practice: Basic Linear Regression Model</h2></a>
        <div class= "well small"> h 15 augusztus 2016

by <a class="url fn" href="https://ernogolya.github.io/author/erno-golya.html">Ernő Gólya</a>
 


 </div>
        <div class="summary"><p>This time I will test and interpret this relationship using basic linear regression analysis for the two variable. Simple linear regression is a statistical method that allows us to describe data and explain the relationship between two quantitative variables</p> <a class="btn btn-info xsmall" href="https://ernogolya.github.io/basic_linear_regression/">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="https://ernogolya.github.io/index.html">1</a></li>
    <li class=""><a href="https://ernogolya.github.io/index2.html">2</a></li>
    <li class=""><a href="https://ernogolya.github.io/index3.html">3</a></li>

    <li class="next"><a href="https://ernogolya.github.io/index2.html">Next &rarr;</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="https://ernogolya.github.io/archives.html">Archives</a>
                <li><a href="https://ernogolya.github.io/tags.html">Tags</a>




            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="https://ernogolya.github.io/category/data-science.html">Data Science</a></li>
                <li><a href="https://ernogolya.github.io/category/machine-learning.html">Machine Learning</a></li>
                   
            </ul>
            </div>




            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="https://hu.linkedin.com/in/ernő-gólya-44626093">Linkedin</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="https://ernogolya.github.io">deviate from the norm</a> &copy; Ernő Gólya 2016</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://ernogolya.github.io/theme/bootstrap-collapse.js"></script>
 
</body>
</html>